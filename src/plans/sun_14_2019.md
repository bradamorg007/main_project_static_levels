# PLAN

# 1.) Use autoencoder research papers architectures to build cnn autoencoder.
    * implement with sigmoid activation.
    * Implemenet sparcity reguliser in keras autoencoder tutorial. apply to VAE dense layers and CNN Autoencoders
    * experiment with uses of drop out aswell. Experiment with and without maxpooling vs stride to reduce feature map size and 
    * implement invariance. 
    * Experiment with upsampling vs deconvolution vs conv2D transposed 
    
# 2.) Build sparcity Measure function
      
      exqution is in the big autoencoder paper it seem straightfoward to compute
      
# 3.) Build small multilayer perceptron for cluster testing

      * create a mlp for 2D, 10D and 30D inputs. architectures are in the papers. the inputs to these
        MLPs will be the latent spaces from the varients of the models I have. If the model has a well formed 
        Latent space then it will have good distinguishable clustrs for each class , making classification 
        Easier. thus MLPs will have lower error and higher accruracy when using the latent space inputs 
        To predict the MNIST class labels. MLP performs classification on MNIST labels using latent spaces rather than images 
        as input. this will give me an understanding of how good and usefull each model is at understanding the data. 
      
      * Use tnse algo to visualise 10D and 30D latent spaces in 2D for plotting
      
# 4.) TRY the Above on my data 
       
      *Figure out a way to label different comninations of gap sizes based on position such as top-mid-bottom e.g
      
      * Or I could use the latent spaces as input to motor system NNs as they are basically MLPs. again models that 
      represent the data well in the latent space should provide good information for the agents to complete the level 
      in less generations than others. Maybe this will work
      
